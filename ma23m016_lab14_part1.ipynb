{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5610a9ec",
   "metadata": {},
   "source": [
    "# Implement the forward propagation for a two hidden layer network for m-samples, n-features as we discussed in class. Initialize the weights randomly. Use the data from the previous labs like logistic regression. You can choose the number of neurons in the hidden layer and use sigmoid activation function.Report the evaluation metrics for the network.  Also use other non-linear activation functions like ReLU and Tanh. Report the loss using both MSE and Cross Entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9a51b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3e763057",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        x1     x2  label\n",
      "0    7.395  7.638      1\n",
      "1    4.987  6.485      1\n",
      "2    5.358  6.499      1\n",
      "3    2.036  2.380      0\n",
      "4    5.956  7.378      1\n",
      "..     ...    ...    ...\n",
      "495  0.304  1.608      0\n",
      "496  6.140  4.261      1\n",
      "497  6.579  6.231      1\n",
      "498  2.555  0.446      0\n",
      "499  2.148  0.852      0\n",
      "\n",
      "[500 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Logistic_regression_ls.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9aeb5859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 2)\n",
      "(500,)\n",
      "[1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 1 1\n",
      " 1 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1\n",
      " 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0\n",
      " 0 1 0 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1\n",
      " 0 1 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0\n",
      " 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0\n",
      " 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0\n",
      " 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0\n",
      " 1 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1\n",
      " 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0\n",
      " 0 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 0\n",
      " 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1\n",
      " 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1\n",
      " 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0]\n",
      "(500, 1)\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "X = df[['x1', 'x2']].values  #datapoints\n",
    "print(X.shape)\n",
    "y = df['label'].values\n",
    "print(y.shape)\n",
    "print(y)\n",
    "y = y.reshape(-1, 1)\n",
    "print(y.shape)\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "648c04dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Loss Functions\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eaf6b0",
   "metadata": {},
   "source": [
    "# For sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c5ceb36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.49435908517682003\n",
      "Cross Entropy Loss: 2.6328122453687772\n",
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Randomly initialize weights and biases\n",
    "np.random.seed(42)\n",
    "input_size = X.shape[1]\n",
    "# print(input_size)\n",
    "hidden_size1 = 16\n",
    "hidden_size2 = 32\n",
    "output_size = 1\n",
    "\n",
    "w1 = np.random.randn(input_size, hidden_size1)\n",
    "b1 = np.random.randn(hidden_size1)\n",
    "w2 = np.random.randn(hidden_size1, hidden_size2)\n",
    "b2 = np.random.randn(hidden_size2)\n",
    "w3 = np.random.randn(hidden_size2, output_size)\n",
    "b3 = np.random.randn(output_size)\n",
    "\n",
    "# Forward Propagation\n",
    "def forward_propagation(X, activation_fn=sigmoid):\n",
    "    # Hidden layer 1\n",
    "    z1 = np.dot(X, w1) + b1\n",
    "    a1 = activation_fn(z1)\n",
    "\n",
    "    # Hidden layer 2\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    a2 = activation_fn(z2)\n",
    "\n",
    "    # Output layer\n",
    "    z3 = np.dot(a2, w3) + b3\n",
    "    a3 = activation_fn(z3)\n",
    "\n",
    "    return a3\n",
    "\n",
    "# Choose activation function\n",
    "activation_fn = sigmoid  # Change to relu or tanh as needed\n",
    "\n",
    "# Perform forward propagation\n",
    "y_pred = forward_propagation(X, activation_fn)\n",
    "# print(y_pred)\n",
    "\n",
    "# Calculate loss\n",
    "loss_mse = mse(y, y_pred)\n",
    "loss_ce = cross_entropy(y, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", loss_mse)\n",
    "print(\"Cross Entropy Loss:\", loss_ce)\n",
    "\n",
    "# accuracy calculation\n",
    "y_pred_class = (y_pred > 0.35).astype(int)\n",
    "# print('The predicted labels are:',y_pred_class)\n",
    "accuracy = np.mean(y_pred_class == y)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "30ef7206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5\n",
      "Recall: 1.0\n",
      "F1 Score: 0.6666666666666666\n",
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def true_false_pos_neg(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def calculate_precision_recall_f1(y_true, y_pred):\n",
    "    TP, TN, FP, FN = true_false_pos_neg(y_true, y_pred)\n",
    "    \n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    \n",
    "    return precision, recall, f1_score, accuracy\n",
    "\n",
    "# Convert predictions to binary class\n",
    "y_pred_class = (y_pred > 0.35).astype(int)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision, recall, f1_score, accuracy = calculate_precision_recall_f1(y, y_pred_class)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a26fe7",
   "metadata": {},
   "source": [
    "# For ReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f60f9489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Logistic_regression_ls.csv')\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8cf3e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['x1', 'x2']].values  #datapoints\n",
    "# print(X.shape)\n",
    "y = df['label'].values\n",
    "# print(y.shape)\n",
    "# print(y)\n",
    "y = y.reshape(-1, 1)\n",
    "# print(y.shape)\n",
    "# print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0fe66b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Loss Functions\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2e8c99bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.4999984924512381\n",
      "Cross Entropy Loss: nan\n",
      "Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Partha Sakha Paul\\AppData\\Local\\Temp\\ipykernel_9504\\2359432263.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
      "C:\\Users\\Partha Sakha Paul\\AppData\\Local\\Temp\\ipykernel_9504\\2359432263.py:16: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n"
     ]
    }
   ],
   "source": [
    "# Randomly initialize weights and biases\n",
    "np.random.seed(42)\n",
    "input_size = X.shape[1]\n",
    "# print(input_size)\n",
    "hidden_size1 = 16\n",
    "hidden_size2 = 32\n",
    "output_size = 1\n",
    "\n",
    "w1 = np.random.randn(input_size, hidden_size1)\n",
    "b1 = np.random.randn(hidden_size1)\n",
    "w2 = np.random.randn(hidden_size1, hidden_size2)\n",
    "b2 = np.random.randn(hidden_size2)\n",
    "w3 = np.random.randn(hidden_size2, output_size)\n",
    "b3 = np.random.randn(output_size)\n",
    "\n",
    "# Forward Propagation\n",
    "def forward_propagation(X, activation_fn=sigmoid):\n",
    "    # Hidden layer 1\n",
    "    z1 = np.dot(X, w1) + b1\n",
    "    a1 = activation_fn(z1)\n",
    "\n",
    "    # Hidden layer 2\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    a2 = activation_fn(z2)\n",
    "\n",
    "    # Output layer\n",
    "    z3 = np.dot(a2, w3) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    return a3\n",
    "\n",
    "# Choose activation function\n",
    "activation_fn = relu  # Change to relu or tanh as needed\n",
    "\n",
    "# Perform forward propagation\n",
    "y_pred = forward_propagation(X, activation_fn)\n",
    "# print(y_pred)\n",
    "\n",
    "# Calculate loss\n",
    "loss_mse = mse(y, y_pred)\n",
    "loss_ce = cross_entropy(y, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", loss_mse)\n",
    "print(\"Cross Entropy Loss:\", loss_ce)\n",
    "\n",
    "# accuracy calculation\n",
    "y_pred_class = (y_pred > 0.35).astype(int)\n",
    "# print('The predicted labels are:',y_pred_class)\n",
    "accuracy = np.mean(y_pred_class == y)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "eec6f075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5\n",
      "Recall: 1.0\n",
      "F1 Score: 0.6666666666666666\n",
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def true_false_pos_neg(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def calculate_precision_recall_f1(y_true, y_pred):\n",
    "    TP, TN, FP, FN = true_false_pos_neg(y_true, y_pred)\n",
    "    \n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    \n",
    "    return precision, recall, f1_score, accuracy\n",
    "\n",
    "# Convert predictions to binary class\n",
    "y_pred_class = (y_pred > 0.35).astype(int)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision, recall, f1_score, accuracy = calculate_precision_recall_f1(y, y_pred_class)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc09ee8a",
   "metadata": {},
   "source": [
    "# For tanh activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3543f89e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Logistic_regression_ls.csv')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "28160c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['x1', 'x2']].values  #datapoints\n",
    "# print(X.shape)\n",
    "y = df['label'].values\n",
    "# print(y.shape)\n",
    "# print(y)\n",
    "y = y.reshape(-1, 1)\n",
    "# print(y.shape)\n",
    "# print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bf19f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Loss Functions\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d7fdaa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.43776735827644925\n",
      "Cross Entropy Loss: 1.6547614587276804\n",
      "Accuracy: 0.448\n"
     ]
    }
   ],
   "source": [
    "# Randomly initialize weights and biases\n",
    "np.random.seed(42)\n",
    "input_size = X.shape[1]\n",
    "# print(input_size)\n",
    "hidden_size1 = 16\n",
    "hidden_size2 = 32\n",
    "output_size = 1\n",
    "\n",
    "w1 = np.random.randn(input_size, hidden_size1)\n",
    "b1 = np.random.randn(hidden_size1)\n",
    "w2 = np.random.randn(hidden_size1, hidden_size2)\n",
    "b2 = np.random.randn(hidden_size2)\n",
    "w3 = np.random.randn(hidden_size2, output_size)\n",
    "b3 = np.random.randn(output_size)\n",
    "\n",
    "# Forward Propagation\n",
    "def forward_propagation(X, activation_fn=sigmoid):\n",
    "    # Hidden layer 1\n",
    "    z1 = np.dot(X, w1) + b1\n",
    "    a1 = activation_fn(z1)\n",
    "\n",
    "    # Hidden layer 2\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    a2 = activation_fn(z2)\n",
    "\n",
    "    # Output layer\n",
    "    z3 = np.dot(a2, w3) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    return a3\n",
    "\n",
    "# Choose activation function\n",
    "activation_fn = tanh  # Change to relu or tanh as needed\n",
    "\n",
    "# Perform forward propagation\n",
    "y_pred = forward_propagation(X, activation_fn)\n",
    "# print(y_pred)\n",
    "\n",
    "# Calculate loss\n",
    "loss_mse = mse(y, y_pred)\n",
    "loss_ce = cross_entropy(y, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", loss_mse)\n",
    "print(\"Cross Entropy Loss:\", loss_ce)\n",
    "\n",
    "# accuracy calculation\n",
    "y_pred_class = (y_pred > 0.35).astype(int)\n",
    "# print('The predicted labels are:',y_pred_class)\n",
    "accuracy = np.mean(y_pred_class == y)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4b7756f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.45357142857142857\n",
      "Recall: 0.508\n",
      "F1 Score: 0.47924528301886793\n",
      "Accuracy: 0.448\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def true_false_pos_neg(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def calculate_precision_recall_f1(y_true, y_pred):\n",
    "    TP, TN, FP, FN = true_false_pos_neg(y_true, y_pred)\n",
    "    \n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    \n",
    "    return precision, recall, f1_score, accuracy\n",
    "\n",
    "# Convert predictions to binary class\n",
    "y_pred_class = (y_pred > 0.35).astype(int)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision, recall, f1_score, accuracy = calculate_precision_recall_f1(y, y_pred_class)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f403ea24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
